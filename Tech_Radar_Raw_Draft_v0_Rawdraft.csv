name,ring,quadrant,isNew,description
Apache Kafka ,Adopt,Platforms,FALSE,"<p>Apache Kafka - Loosely coupled and highly cohesive Microservices</p>
<p>Microservices architecture adoption is notentirely realised if the coupling among the components is not carefully designed. Any microservice that invokes services directly from others is implementing one of the worst integration patterns, point-to-point communication. <br />This integration pattern does not provide interface abstraction and creates a strong functional coupling. <br />Our primary objective in extensive work or large enterprise ecosystems is to implement an event-driven architecture. We consider Kafka the best foundation platform for it. In turn, it enables us to implement other correlated architecture patterns, such as eventual consistency and CQRS. <br />From the architecture viewpoint, the best outcome is for microservices to have low functional coupling because it allows updating and deploying each without disturbing the rest. <br />These patterns are not just techy desirables; they are highly beneficial to the business since they help minimise the implementation and the test cycles, improving the overall time to market.<br />Regarding development productivity, the patterns achieved with Kafka contribute to organising parallel development for multiple domains and services by defining the event contracts before starting coding. <br />Once the real events are available, it is possible to start composing the real events and flow through Kafka topics. The sequence of events creates a composable choreography of business scenarios, still achieving an overall system loosely coupled and highly cohesive.</p>
<ul>
<li>There are many architecture patterns and use cases where we can leverage Kafka&rsquo;s pub-sub and streaming capabilities:</li>
<li>Microservices choreography and orchestration</li>
<li>Event-driven architecture</li>
<li>Capture and analyse data from IoT devices or other equipment</li>
<li>Eventual consistency</li>
<li>Process payments and other business transactions in real-time</li>
<li>Stream processing and data transformation</li>
<li>Event sourcing and log aggregation</li>
</ul>
<p>These will be discussed in detail in future blips.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>"
Chakra Ui,Trial,languages-and-frameworks,TRUE,"<p>Chakra Ui</p>
<p>The front end is a crucial part of every project; after all, it's the first thing the user sees and your only chance to make a great first impression. <br />If the front end is not user-friendly, sluggish, or doesn't fit within the overall context, no one will care about APIs, backend components and even what data is being shown. So, in any web project, it is primordial to focus on delivering great UXs, with quality and responsiveness. <br />Creating custom UI components from scratch takes time and resources.<br />We currently consider Chakra Ui one of the best solutions for the Ui component for React. <br />Each of Chakra UI's components is approachable using WAI-ARIA standards, which we believe is a hot topic for most of us dedicating our time to design and build UIs.<br />Chakra UI's components are simple for editing, expansion, and theme and small and easy to combine to construct larger structures. Chakra UI provides more styling and customization ability than other React UI libraries like React Bootstrap, Material UI etc. Also, If you like Tailwind CSS, you'll enjoy Chakra UI because it uses the same minimalistic and modular approach as Tailwind CSS.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>"
Coder remote development environment,Adopt,Platforms,TRUE,"<p>Coder remote development environment</p>
<p>The Coder remote development platform allows having a development desktop machine running on a remote server, and the local desktop machine is only used for remote access.<br />The Coder remote development platform is an open-sourced alternative to Github codespaces. This technology addresses many constraints of using a local machine for a development environment. <br />Coder is open-sourced and licence-free if self-hosted.<br />One of them is to keep the data and the codebase under a firewall while allowing third-party developers to collaborate in development activities in a trusted environment.<br />Another is allowing developers to have more than one machine set up with different projects, tools, operating systems, and customisations without having multiple booting set up in a local machine.<br />The ramp-up time for new developers is also reduced drastically in these remote development environments.<br />Another important thing is that the remote machines' specifications can be customised for the occasion, with more RAM or CPU.<br />In our case, we find that Web3 developers need multiple tools and setups for developing projects. In Web3, many protocols, frameworks and networks need to be set up on these machines, which increases the time of setting up and creating these environments. One good alternative is to set up a remote desktop environment for each project, so there is no need to deal with configuration and incompatibility problems.</p>"
Contract Testing,Adopt,Techniques,TRUE,"<p>Contract Testing</p>
<p><br />In situations where we are working on a large system and creating microservices, web applications, and mobile applications, every time we update one of these components, we are disturbing the system. The testing is complex and time-consuming, especially regression and integration testing. Most of the time, the end-to-end integration tests do not serve well when the systems are highly distributed and complex since they miss many scenarios because of the possible permutations of the messages. Also, preparing the test data becomes a significant burden. <br />As a result of this need, a new category of testing was conceived. The contract testing addresses the verification of interfaces in large, highly distributed systems.<br />Contract testing ensures that two separate systems (such as two microservices) are compatible and can communicate with one another. It captures the interactions between each service, storing them in a contract, which can later they are used to verify that both parties adhere to it.<br />Contract testing proposes an exciting value proposition. Generally, when working in large distributed systems, this testing is cheaper and more convenient than the end-to-end. Therefore, they are relatively inexpensive and faster than end-to-end testing, which blocks a complete environment. <br />Tools like &ldquo;pact&rdquo; from https://pact.io have become popular to minimise the need for end-to-end testing. For example, writing a pact contract test for the scenarios of a contract between a publisher and consumer service is as simple as specifying which schema they agree upon using between them. <br />As per our experience, there is also a relatively small learning curve and one-of setup effort to be considered<br />Pact is a little challenging to set up, but it is the preferable open-source version on the market. It is required to set up the interaction server and scripts to execute the publisher and consumer test cases. It will not take more than 5 to 10mins to write the contract tests, which is remarkably similar to writing a Unit-test case. The whole setup and your first contract testing will not take more than 3hrs.</p>"
DAPR (Distributed Application Runtime) ,Trial,languages-and-frameworks,TRUE,"<p>DAPR (Distributed Application Runtime)</p>
<p><br />In the past, developers spent much of their time controlling the environment and infrastructure rather than writing code that addresses business requirements. <br />In any normal project situation, it is required to address the configuration of platforms and infrastructure before starting coding business stories. It seems impossible to postpone some decisions to a later stage because the libraries and frameworks required for each platform also condition how the code is organised and coded. E.g. It would be detrimental to the timeliness of the project to swap Rabbit MQ for Kafka after you have implemented the interfaces to work with Rabbit MQ. <br />This problem gets even more prominent when we have many microservices and want to change the underlying platform protocol. <br />So, this is when Dapr provides solutions to this problem of technical platform isolation from the business codebase. <br />Now let's dive into the basis.<br />Dapr is a free, open-source runtime system designed to support the rapid development of cloud-native and serverless computing.<br />Dapr is designed to be deployed as a sidecar to the central business microservices and is intended to be a technical helper sidecar. Following the sidecarpattern, it gets attached to the business logic runtime and takes the responsibility to do the technical extra mechanical task for you. These can be to provide the pub/sub behaviour, implement distributed cache, manage all the secrets of the application, service discovery, health monitoring of the applications, telemetry logging, and the list continues.<br />Dapr codifies the best practices for building microservice applications into open independent building blocks that enable you to create portable applications with the language and framework of your preference. Each building block is separated and disconnected; the developer can use one, some, or all in the application. Dapr injects a sidecar (container or process) into each compute unit. The sidecar interacts with event triggers and communicates with the main microservice runtime via standard HTTP or gRPC protocols. The separation of the runtimes through standard protocols enables Dapr to evolve, be upgradable, and be readily available for any language of choice because its communication is via HTTP or gRPC, running itself in its dedicated runtime.</p>
<p><br />Features of DAPR:</p>
<ul>
<li>Event-driven Pub-Sub system with pluggable providers and at-least-once semantics</li>
<li>Input and output bindings with pluggable providers</li>
<li>State management with pluggable data stores</li>
<li>Consistent service-to-service discovery and invocation</li>
<li>Opt-in stateful models: Strong/Eventual consistency, First write/Last-write wins</li>
<li>Cross-platform virtual actors</li>
<li>Secrets management to retrieve secrets from secure key vaults</li>
<li>Rate limiting</li>
<li>Built-in Observability support</li>
<li>It runs natively on Kubernetes using a dedicated Operator and CRDs</li>
<li>Supports all programming languages via HTTP and gRPC</li>
<li>It runs anywhere, as a process or containerised<br />It runs as a sidecar - removes the need for special SDKs or libraries</li>
<li>Dedicated CLI - developer-friendly experience with easy debugging</li>
<li>Clients for Java, .NET Core, Go, JavaScript, Python, Rust, and C++</li>
</ul>
<p>Writing high-performance, scalable, and reliable distributed applications is challenging. Dapr helps by bringing proven patterns and practices and supports all programming languages without framework lock-in. The developers implementing business logic are not exposed to low-level primitives such as threading, concurrency control, partitioning, and scaling. <br />One of the custom use cases we implemented using Dapr is to create a util error handling helper sidecar. This sidecar is provided with gRPC and HTTP connectivity support using Dapr. It can be deployed to any main business microservice runtime, and it will attend to any request to handle errors in the application. <br />The communication with the Dapr-based sidecar is through gRPC protocol, which consists of the protobuf format, and also HTTP can be used. The business logic won't be affected if anything goes wrong with the sidecar. <br />For further diving into Dapr, visit the following website https://dapr.io/.</p>"
Domain-driven design for Microservices modelling,Adopt,Techniques,FALSE,"<p>Domain-driven design for Microservices modelling</p>
<p>Although the Domain-Driven design paradigm was created before the microservices, it is a paradigm that goes hand in hand with it. In the microservices world, modelling the correct granularity for a domain is complex, and Microservices could end up being too thin or too fat.<br />So, in this regard, the DDD provides principles, techniques, and patterns that help to understand the processes, functions and business rules of business domains, which will be converted into services and logic in the business domain microservices. <br />Using Domain Driven design and its consequent development Domain Driven design approach will fit best in large and complex projects requiring domain experts' involvement to get their knowledge and convert it into product features.<br />The DDD approach allows dissecting domain knowledge and modelling the collective understanding before the implementation begins with the support of domain experts. <br />Using the correct artefacts that describe the problem space can create a perfect bridge of understanding between the business stakeholders and the technical team.<br />While working with Domain-driven design, we focused on the following principles and steps:</p>
<ul>
<li>Focus on finding and establishing the boundaries of the Domain.</li>
<li>Focus on the understanding of the core logic embedded in the Domain.</li>
<li>Propose a Domain model and test it with real examples and scenarios.</li>
<li>Find the collaboration between the business domains.</li>
</ul>
<p>The team will start getting familiar with some of the terms used in DDD: Domain, Subdomain, Ubiquities language, Bounded context, Aggregate, Value Objects, and Entities.<br />From our experience working with Domain experts, it is essential, while making these models, to keep close contact with experts who understand the real-world version of the system we are building. Also, treat the domain expert as your co-developer; they know the problem in detail but do not know how to code. <br />About the modelling techniques, when creating the models, make the models in a way that solves real-world problems in the most straightforward way possible. Follows the experts break down at first to understand and avoid not over factorise the business entities found. <br />Later during the modelling exercise, review all the business domains and entities found, and balance the business entities in equal responsibility. So they are not too small or too big and avoid anemic domains. When the system grows, there will be opportunities for refactoring and redistributing responsibilities.<br />When refining the microservices boundaries, remember that the most powerful feature of this architecture comes from the benefits of isolating deployments and changing parts of the system without affecting other components. <br />So, it is advisable to put in perspective the work we are progressing in terms of the benefits we want to achieve. Based on this perspective, it is required to forecast what parts of the system will most likely change simultaneously and refine their responsibilities, boundaries and interactions.<br />It is a fact that the real world and the business evolve, and some taken assumptions may be realised orchanged. Hence, the team must consider having business owners' support and the skills to adapt, alter or refactor domains; this is another critical methodological principle to keep the overall system balanced and maintainable.<br />Regarding the challenges and the potential pitfalls you could expect to face when embracing DDD for microservices, we can mention that multiple teams have an upfront collaborative effort for the analysis and design phase. To justify this, consider that the benefits obtained from DDD must outweigh the effort required to do the modelling and design. <br />Also, the business experts and the technical team will need to collaborate during this phase, which needs to be streamlined. The best solution for this is adopting the Event Storming technique. During these sessions, there is structured communication between developers and domain experts to create models and bounded contexts or boundaries. <br />During the Event Storming sessions, the objective is to brainstorm about the business domain understanding and identify the various sub-domains, the boundaries, the events, the business entities that will conform to the data model, the aggregates, value objects, etc., also map the events, commands, and processes. <br />The technical team also have the opportunity to ask questions and identify the potential overlapping of the concepts. At this stage, the business processes and the domain language are coming to light. <br />As this exercise progresses, the team also identify the bounded context to which the aggregate belongs.<br />The setup of the Event storming workshops is meant to be done on-site with all the team members in a single place. But we found that it is possible to do it also virtually through a video conference if everyone only focuses exclusively on the session.<br />As a result of these workshops, the developers start having break-up sessions and reflect on the business domain understanding and how these will be translated into code. The developers must design the Domain layer in code that describes the business logic using DDD terms and ultimately into a domain language.</p>"
Flutter ,Adopt,languages-and-frameworks,FALSE,"<p>Flutter</p>
<p><br />Flutter is a new hybrid framework for mobile development from Google, and it has quickly caught on among developers and businesses. With Flutter, it is possible to compile applications to target Android and iOS from the same codebase targeting both markets with a single project team.<br />We have tried Flutter, and although hybrid frameworks are not new, we see that Flutter is growing in popularity and support because it is modernising the overall hybrid development approach of mobile application development. <br />The Flutter framework is quite capable, but it is still relatively new and could come with some functional limitations, so you should be aware of it before committing to Flutter. The workaround we found, for the time being, is that sometimes you would need to consider using some SDKs or native libraries support in the application when facing these limitations.We can mention a few points for reference from the developer's perspective. <br />The set-up is relatively quick; it takes less than 30 minutes to finish the installation process and set up a working development environment. <br />Popular Editor integrates Flutter into the IDE. We try Android Studio, but instructions are available to set up IntelliJ, VSCode, or Emacs.<br />The development process is relatively smooth compared with the experience of another hybrid app framework we used in the past, and it feels cleaner and better organised. For example- if I needed to show lists with many elements, I had to deal with high CPU usage, especially when the user was scrolling the list. In Flutter, one accepted way of displaying lists is the ""ListView"", which has impressive performance outside the box.<br />The cross-platform UI consistency across both mobile platforms is remarkable. Flutter uses its rendering engine, so the UI is the same on iOS and Android. Developers no longer must grapple with platform-specific styling issues to get everything looking exactly like the design. The framework guarantees UI compatibility.<br />Another exciting feature is Live Reloading; it reflects the changes immediately on the screen when the developer writes the code, similar to how the browser rendering works. However, it does not have an embedded browser or another rendering similarity. This feature also demonstrates its performance, which looks insane, because it is converted into native mobile code specifically for Android and iOS. So, this feature shows something special that is not easy to achieve. Older frameworks must be compiled and deployed, so it is impossible to see code changes immediately reflected in the UI<br /> Lastly, if you want to use the full capabilities of Flutter, then you consider knowing and following Flutter's best practices to help develop your first app smoothly.</p>
<ul>
<li>Modularise the application code.</li>
<li>State management.</li>
<li>Separate the View, Models and Services management.</li>
</ul>"
GraphQL,Adopt,languages-and-frameworks,FALSE,"<p>GraphQL</p>
<p><br />GraphSQL is a query language to retrieve data selectively from a domain on demand. We have adopted GraphQL in many projects. Our experience is excellent, and it fits well when needing to have the same API endpoint and return dozens of fields for an application, for example, mobile apps. However, it does not fit well for other scenarios where we only need to query a few fields in the API payload. <br />GraphSQL also has trade-offs and decisions to be made, for example, when it is a suitable time to create a new API endpoint. <br />Sometimes it is good to build a new dedicated API for a specific scenario or requirement that will only return these fields.<br />On the other hand, we could also modify an existing API and accommodate these fields in a current API. The decision is not only about the build effort but also the performance of the APIs. The more data fields are accommodated into an API, the less performant it becomes. We must be cautious of the performance penalties when using GraphQL with many fields in the same API because we are either returning or filtering unnecessary fields. <br />Also, consider reviewing the scalability and implementing some data caching when dealing with these scenarios, as it uses a single endpoint.<br />We think that also fits well when we have a single endpoint serving multiple clients. So, each client queries the same or different information from a business domain using the same API. So that GraphQL serves the primary purpose because it is a generic query language that permits anyone to get what each client needs from a business domain.<br />Regarding data consumption, the format of the response payload is client-oriented, as the client drives the structure of the structure from the request parameters. <br />GraphQL uses strong data types to ensure that apps can only ask for what is possible and provide clear and helpful errors. <br />The GraphQL design forces the server to follow Postel's law: ""Be liberal in what you accept and conservative in what you send.""<br />GraphQL uses a single endpoint for all the requests by defining query and mutation. So, it is fast to create new functionalities in GraphQL, minimising overall market time. <br />From the development experience, the technology is mature enough that provides good development tools to call the GraphQL queries and make use of the possible payload mutations by providing assistance with good IntelliSense and embedded online documentation.</p>"
Hexagonal architecture ,Adopt,Techniques,FALSE,"<p>Hexagonal architecture</p>
<p>On the one hand, microservice architecture solves many architectural problems. It addressed existent inefficiencies, from infrastructure hosting to application modularisation. On the other hand, it introduces new issues that need addressing. Developing structural support for technical capabilities as the hexagonal architecture helps maximise the benefits of the microservices architecture and, at the same time, helps minimise the downsides.</p>
<p>Full document<br />Hexagonal architecture <br />It is a design pattern whose main objective is to separate concerns about the business logic code base and the technical code base. This architecture allows the creation of an abstraction layer where all the input and outputs, and auxiliary functions, are separated from the core domain logic.<br />A hexagon shape represents this architectural pattern, with a business component in the core surrounded by the services on the six sides. <br />This pattern is mainly used for application architectures and allows the creation of &ldquo;ports&rdquo; or &ldquo;adapters&ldquo; for the business domain in the core. These services are integrated with the core domain by using a defined interface. The design of these interfaces should be technology agnostic, and they should not constrain any language. So, it is possible to evolve the services without affecting the core. <br />The overall idea is to have composable capabilities that can be reused. In addition, in most cases, these capabilities have become deployable units that can be utilised within business domain microservices, applications, or just stand-alone capabilities. These capabilities can take the shape of Libraries, Services, or sidecars. Nowadays, modern trends try to formalise these patterns as frameworks. The most known are Dapr and CloudState.<br />This architecture allows late binding, postponing the implementation or deployment decisions to later stages of the development cycle. <br />In terms of advantages and disadvantages, Hexagonal Architecture aims to increase the codebase&rsquo;s maintainability by decoupling the core code from the rest, the packaging strategy, and the isolation of testing functionality. Regarding disadvantages, it adds some complexity to the build and debugging time; it may add latency because of the extra hops between the added abstraction layers. <br />The services implemented in Hexagonal Architecture can be classified into three categories. Microsoft just started providing container services with built-in Dapr capabilities. However, not all the hexagonal architecture capabilities proposed in this document are covered by Dapr nowadays.</p>
<ul>
<li>&nbsp;Domain Services<br />They are components that are responsible for implementing the business domain logic. They follow the Domain-Driven Design (DDD) taxonomy. e.g., &ldquo;Aggregated root&rdquo;, &ldquo;Value Object&rdquo;, etc.</li>
<li>&nbsp;Application Services<br />They are components that are responsible for orchestrating the execution of domain logic. They conform to a layer with a defined interface that interacts with the business domain tailored per scenario.</li>
<li>Framework Services<br />Framework or Infrastructure Services are underlying components that contain the technology needed to run the Application.</li>
</ul>
<p>The hexagonal architecture is not an exclusive design for microservices. The hexagonal architecture is an architectural pattern that Alistair Cockburn first detailed in 2005. The deployment options for designing these pluggable capabilities were not advanced; therefore, using shared libraries was the only option for implementing these. It is difficult to determine if this was a predecessor of the microservices architecture. The microservices architecture became mainstream, and at the same time, the runtime deployments evolved. Kubernetes allows having a microservice with multiple runtimes. One is dedicated exclusively to the business domain or logic, and the other is for utility services as required.<br />From the viewpoint of Hexagonal Architecture as a business enterprise, this application taxonomy opens the possibility of creating the most optimal reusable components ever designed up to this time. On one side, these util capabilities can be designed in different languages, runtimes, and technology options. They are completely isolated from the business microservice and can evolve independently from any other. In addition, good versioning and deployment practices can be deployed and recycled without affecting the runtime of different components.<br />These components provide a function through a standard interface; therefore, the development of business capabilities should not interfere with the many architecture options of a project; these technology decisions can be delayed until later stages. Moreover, even the deployment options can be delayed until later stages. This design allows several alternatives for deploying these utility services, and these can be:</p>
<ul>
<li>&nbsp;Embedded shared libraries in the business microservices runtime.</li>
<li>&nbsp;A business microservices sidecar.</li>
<li>&nbsp;Complete independent components.</li>
</ul>
<p>So, in conclusion, investing in building components using this application architecture taxonomy seems to open a new economy of scale for software development, where the pieces, if they are well designed, could be reused on a large scale.</p>
<p>In the microservice architecture that comprises a business domain and other technical capabilities, it is possible to separate the critical business logic from different codebases using the hexagonal architecture principles.<br />Business Domain<br />A business domain defines the area of operation in an application or system. Domain-Driven Design is a sphere of knowledge, influence, or activity that can be conceptualised as a subject area to which the user applies for a program as the software domain. A DDD domain is a combination of the areas of knowledge (knowing what will happen with some data or events and typically the section in which you or your Application have the primary business perspective), influence (like impacting your business with your actions or activities). Activity (Do specific tasks necessary with your knowledge and with whom you cause effect in your business area).</p>
<p>The technical capabilities that could be considered in the hexagonal architecture are the following:</p>
<p>Caching Services<br />It is a component that provides the necessary methods for lifecycle content in a caching database. This database is faster than on-disk databases because they keep the content in memory, increasing throughput and lowering data retrieval latency.</p>
<p>Connectivity Services<br />It is a component that implements the integration services and other primitive functions and provides these services to the business microservice. It abstracts protocol specifics, technical connectivity and retries, and message data formats.</p>
<p>Document Storage Manager (DMS)<br />It is a module responsible for managing all types of images, documents and other types of structured and semi-structured collateral information collected as part of the normal operations of the Application.</p>
<p>Entitlement Manager<br />It is a component that is responsible for granting, resolving, enforcing, revoking, and administering application and services fine-grained access entitlements (also referred to as &ldquo;authorisations,&rdquo; &ldquo;privileges,&rdquo; &ldquo;access rights,&rdquo; &ldquo;permissions&rdquo;, and &ldquo;rules&rdquo;).</p>
<p>Error Handler<br />Error handling refers to the process comprised of anticipation, detection and resolution of application errors, programming errors or communication errors. It covers response and recovery procedures from error conditions present in a software application.</p>
<p>Event Streaming<br />It refers to the technology that makes it possible to transform discreet input and output data units into a continuous stream. They can be consumed by one or many event topics, producing an event join condition. It only processes the delta records from the last run.</p>
<p>Integration Services<br />It is a component that will centralise the integration with External Parties. It will contain all the data required to be autonomous and consist of sub-modules specialising in different protocols and flows. e.g., Events, HTTP, Files, Incoming, Outgoing, Batch, Synchronous, Asynchronous, etc.</p>
<p>Integration Services - Event Broker <br />It is a module part of Integration Services and pushes data from the central system to External Systems. It allows capturing data changes on internal events and invokes call-back-URLs from External Systems</p>
<p>Integration Services - ETL (Extract Transform and Load)<br />The ETL is part of Integration Services. It retrieves (Extracts or Pulls) data sets from the External Systems, Transforms them, and Loads them into a central system. These External Systems should have been registered in our Integration Portal.</p>
<p>Logging services<br />In a distributed system, each service generates its log trail. There could be transactions with problems that may include more than one service. Therefore, it is required to have a solution for standardising the lifecycle of logs, consisting of the logging format, local capture, shipping of the records, aggregating logs from each service instance, and facilitating the analysis of records. It is based on commodity technology: Fluentd, Open Search (a.k.a. Elastic Search), and Kibana Dashboards.</p>
<p>ORM Services<br />Object-relational mapping (ORM) is a design technique for converting data between an application and the database engine using an object-oriented programming paradigm. It provides the effect of a &ldquo;virtual object database&rdquo;, which facilitates the development of a business domain and business logic. In addition, it can provide extra benefits when implemented as a library or a sidecar. It abstracts all access and connectivity with the database. It follows the principles of separation of concerns based on domains and decentralised ownership by applying discrete responsibility to self-contained contexts as microservices. When referring to the microservices data, it creates a &ldquo;data mesh&rdquo; layer, a concept related to enabling &ldquo;data as a product&rdquo;. It conforms to distributed taxonomy and can be used by data analytics consuming &ldquo;data as a service&rdquo;.</p>
<p>Outbox dispatcher<br />It is a design pattern that implements a Transactional Outbox, one of the best approaches for solving the &ldquo;Dual Write&rdquo; problem.<br />The Dual Write problem for microservices is when they need to maintain their state private to them and then notify that change to a broader audience. So, this pattern aims to solve these two steps to be executed as a single task atomically. It is implemented as a component responsible for tracking the changes in the microservices&rsquo; database. In particular, the table is called &ldquo;Outbox&ldquo;. The Outbox table will store documents that will have the changes done in the business schema, and these payloads will be published as Events in the Business Domain Topic.</p>
<p>Middleware <br />Allow intercepting and composing activities before and after the flow reaches the microservice.</p>
<p>Pub-Sub<br />Publisher &amp; Subscriber is an integration pattern where components called publishers place events (messages) on queues (or topics). And other system components called Subscribers consume these events. The Publishers and the Subscribers don&rsquo;t know the existence of each other. The integration pattern allows message filtering, and this is when the Subscribers decide to receive only a subset of the total messages published. There are two common forms of filtering: topic-based and content-based. In a topic-based system, the messages are published to &ldquo;topics&rdquo;.</p>
<p>Reference Data Manager <br />Reference data is data used to classify or categorise other data. Typically, they are static or slowly changing over time. And the Reference Data Manager is a component responsible for managing an application&rsquo;s reference data and simplifying how this data is shared across other components and systems.</p>
<p>Rule Engine - Business Rule Engine (BRE) <br />It is specific software that allows defining, analysing, executing, auditing and maintaining a wide variety of business logic, collectively referred to as &ldquo;rules.&rdquo; It enables business stakeholders to keep updated with the rules using a simplified user interface to configure decision trees, decision tables, etc.</p>
<p>Session Manager <br />It is a component responsible for the lifecycle management of the user session. The session is created after the user authenticates. For each API invocation, the system will validate if the session is valid before the flow reaches the Application. After some inactivity or expiration time, the session will become invalid.</p>
<p>Workflow Engine - Workflow (microservices-workflow)<br />It is an end-to-end orchestrator of activities. The components of a system can be a participant in an end-to-end flow organised by a predetermined and organised business flow that allows tracking of a sequence of activities from start to end. In the context of microservices, the workflow invokes microservices components, therefore creating an orchestration pattern. Microservices are designed decoupled from one another. So, the orchestration pattern must be carefully planned and adapted to the microservices principles. It converts workflow orchestration into a loosely coupled integration between components. This type of workflow loosely connecting the microservices is a pattern known as &ldquo;microservices-workflow&rdquo;.</p>"
Kong API Gateway,Adopt,Platforms,FALSE,"<p>Kong API Gateway</p>
<p>With the adoption of micro-services architecture, it is usually found that other patterns should be considered simultaneously. In particular, the API Gateway. It is a built-for-purpose reverse proxy to deal with restful APIs traffic before sending the request to the microservices. It protects a nominated network by creating perimeter security and an exclusive access point. <br />The API Gateways can be generic traffic verifiers. However, choosing the correct API Gateway (Kong API Gateway) can open the possibility of creating a very agile environment where the API Gateway is a fundamental piece of the security strategy. It is a model where the application security is designed and implemented on layers. On this, the API Gateway is given many more responsibilities than the mere traffic verifier. It plays a significant role in the application and enterprise security strategy, which otherwise would have to be done in a secondary layer. It is strategically located at the perimetral network level, so the idea is to use it to do several security verifications of different kinds rather than pass the traffic stream to downstream applications.<br />Security in microservices is required, but it is difficult to enforce it. One possibility is duplicating the same code for each micro-service as a shared kernel library. This model is good but not optimal. So, with Kong API Gateway, the authentication and authorisation verifications are done by default to all APIs invocations before the traffic reaches the microservices. <br />It also provides many other out-of-the-box plugins for load balancing, logging, authentication, rate limiting, request size limiting, transformation, IP whitelisting, etc. It is also possible to extend these security verifications by implementing custom plugins. There is a deployment version of Kong that takes the role of Kubernetes ingress controller.<br />So, if you are using Kubernetes architecture, it can help you take full advantage of the power of the Kubernetes framework. Generally, a Kubernetes cluster has only one ingress controller. In this case, we are defining a type of cluster secured by design since only HTTP restful APIs traffic can reach application components.<br />The Kong API Gateway plugin list is extensive, including the custom-developed plugins we use in our baselined architecture framework. Below we all the add-ons provided by Kong and the others we added to the list. These are not meant to be used simultaneously but are configured accordingly, depending on the scenario.</p>
<p>CORS<br />Cross-origin resource sharing (CORS) is an add-on that allows configuring restrictions to the original applications invoking the APIs. It controls the cross-origin HTTP requests initiated by clients that do not conform to the same domain where the APIs are deployed. If the REST ' 'API's resources receive cross-origin HTTP requests, they will be rejected.</p>
<p>IP White-listing<br />This add-on allows a list of IP addresses or a range to invoke APIs. It is a valuable capability when integrating business-to-business (B2B), where the connections between systems can be predefined.</p>
<p>Bot detection<br />It is an add-on that protects an API endpoint from being used by a Bot. it recognises invocation patterns using the most popular algorithms used by bots. It has the capability of whitelisting and black-listing clients temporarily or permanently. JWT Validation (Local check to verify the token has not been tampered with or expired). It is an add-on that checks requests containing HS256 or RS256 signed JSON Web Tokens (as specified in RFC 7519). It confirms the existence, integrity and expiration of the token. It is a local validation; therefore, it does not introspect the content.</p>
<p>Rate limiting<br />It is an add-on used for configuring and validating the number of HTTP requests and the frequency of a client. It works by defining Time Windows, and these periods can be seconds, minutes, hours, days, months, or years.</p>
<p>Correlation Id (Add correlation id)<br />The correlation id field identifies every incoming request to the system univocally. From an application, perspective is used for logging, traceability and operation support. From a security perspective, it is used for non-repudiation and auditing. The add-on verifies if the incoming HTTP Request contains the required Correlation Id. If the correlation id is not in the header, it generates a new one and adds it to the payload.</p>
<p>Payload Syntax validation<br />It can be done using an out-of-the-box plugin or creating a custom one.<br />See more: https://docs.konghq.com/hub/kong-inc/request-validator/</p>
<p>Payload Semantic validation<br />It can be done by creating a custom plugin with a configuration file per endpoint. This type of library implements a specialised semantic rule engine. Easy to configure and to do as part of an API definition<br />See more: https://www.npmjs.com/package/jsontron</p>
<p>Web Application Firewall<br />If there is not a forefront WAF implemented on the incoming traffic, it will be required to implement a custom plugin that connects to a WAF instance.<br />See more: https://owasp.org/www-community/Web_Application_Firewall</p>
<p>JWT session token validation<br />It is a plugin. It validates the session against the Login Service, which lifecycle manages the session.</p>
<p>JWT session token authorisation validation<br />It is a custom plugin. It validates the Token Claims &ndash; and scopes &ndash; against the User Access management component, validating the Routes and Application User Roles and the APIs they are entitled to use.</p>
<p>Identity Aware plugin<br />This plugin will aggregate the invocations to the RBAC and the Session management capabilities.</p>
<p>RBAC (Role-Based Access Control)<br />This capability checks the roles or privileges granted to the User to use the API. It can be implemented in a custom plugin and calls an underlying service that brings the following information: Endpoint -&gt;ApplicationRole-&gt;UserIdentity. <br />The privilege of using an API endpoint is granted in the context of an application and uses the ' 'Application's User Roles provisioned to the User.<br />See more in the User Access Management microservices.</p>
<p>ABAC (Attribute-Based Access Control)<br />Attribute-Based Access Control. It is used to verify the content of the message. This capability provides security to incoming requests. It prevents the attack scenario when someone with a valid session token has enough skills to handcraft a payload and sends a handcrafted HTTP request to bypass the 'application's security. In this attack, data that is not ordinarily available to the User through the application can be leaked.See more about Kong API Gateway in the Technology Infrastructure section of this document.</p>
<p>Identity Aware verifier<br />It adds intelligent policy-based verifications to API calls&mdash;for example, IP addresses (geolocation), client machine MAC addresses, application destination frequency, Time of access, etc.</p>"
Micro frontends,Assess,Techniques,FALSE,"<p>Micro frontends</p>
<p>Micro frontends are known as microservices for the front end. <br />We adopted micro frontends as the default way of breaking the conception and delivery of UX components. We think the benefits of adopting microservices architecture are not entirely realised if there is a monolithic single-page application where all the data must be composed by a single team. So with the micro frontends, the decoupling among the components is going to the next level. If this is done correctly, the micro frontends components, the APIs and the microservices are owned by a single team.<br />One of the most challenging patterns to change is the APIs that are called from the single-page applications; now, they can only be called by a specific team that owns the micro frontend component. The practical implication of this policy is that there is no aggregation of data by calling multiple APIs by the application. So the only way of passing data back and forward in the application is by using parameters through DOM events.<br />Micro frontends and the architecture policies mentioned above could sound like many hassles. So, there must be substantial benefits from achieving the micro frontends, not only from the technology viewpoint but also from the governance perspective. <br />The motivation is clear and tangible in large environments where these micro frontends can be used in more than one application (or embedded in more than one page). It is achieving the full verticalisation of the application architecture to dissolve the dependencies and the monolithic. Each team can work with their business owner separately and independently. <br />Now the business and the team behind it can own the whole stack from the user interface, the APIs, and the microservices backend components. <br />So, jointly with micro frontends, microservices can help reverse Conway's Law, which states, ""Organisations, who design systems, are constrained to produce designs which are copies of the communication structure. "". <br />Adopting the two creates the ultimate verticalisation in the architecture and the organisation, and it is the maximum end-to-end independence possible for teams to achieve business agility.</p>
<p>&nbsp;</p>"
Microservices,Adopt,Techniques,FALSE,"<p>Microservices</p>
<p>Adopting microservices is not a development or a technical decision but a business decision. The microservices architecture is an answer to breaking down monolithic applications or components. Monolithic architecture's worst problem is that all the applications or all the components consist in a single code base which requires testing and the deployment of the whole application, even if there is a minor update. So best of the benefit of microservices is that the deployment nature allows a targeted feature deployment.<br />We have this benefit in mind when approaching the design of microservices, so the approach is that anything that changes together tends to stay in the same codebase. So the business can benefit from this architecture since they can propose changes to the application, and the requirements and the implementation cycles are faster from idea to realisation.<br />Other problems are solved with microservices; tight coupling in technical components results in interdependency among teams. So changes require more coordination and preparations. Also, if all the components are in the same codebase, they must be using the same language. So there is an implicit lack of flexibility in choosing technology. In terms of scalability, it is impossible to separate those components that are used more than others. So the application needs to be scaled up entirely. And this produces lots of inefficiencies. Regarding regression testing, the application must be quality assured completely, instead of the components that changed. Thus more effort in the development life cycle. <br />On the other hand, microservices overcome these limitations by creating loosely coupled services that can be developed, tested, deployed, maintained, and scaled independently. It enables the development team to work in parallel without blocking each other. The team has the freedom to choose the best-suited technology stack for each service. Each component provides fault isolation. Therefore, there could be different SLAs to achieve different availability targets, thus making the system more resilient. Also, there are fewer regression test issues since components are compartmentalised. <br />However, microservices architecture has some drawbacks. For example, to mention some of the ones we confronted, the communication among micro-services could end up in a big, intelligible mess. <br />Autoscaling, distributed tracing, health checks, and data aggregation. <br />So, it requires the right technology, architecture patterns, and several iterations to get the micro-services right.</p>
"
Microservices workflow ,Adopt,Techniques,FALSE,"<p>microservices workflow</p>
<p>On the one hand, microservice architecture solves many architectural problems. It addressed existent inefficiencies, from infrastructure hosting to application modularisation. On the other hand, it introduces new issues that need addressing. <br />One of the most significant issues is the organisation of long-running transactions involving multiple microservices. These can be part of synchronous or asynchronous workflows, and the long-running transactions can also be with human tasks or fully automated. In summary, having a clear design strategy and technology foundation on how microservices collaborate is critical in large environments. Otherwise, it creates antipatterns. <br />These could be:</p>
<ul>
<li>If no organisation is put in place, then the microservices will call each other without no control, creating a big ball of invocations that are challenging to trace</li>
<li>If using one microservice as the orchestrator, then it recreates the ESB orchestration antipattern</li>
<li>If using the event-driven architecture with the saga pattern, then after the second event, it is difficult to trace and operate in case of errors.</li>
</ul>
<p>The ""Microservices workflow"" can be defined as a design pattern that addresses the organisation of long-running transactions involving multiple microservices. This design pattern is in between orchestration and choreography, and it does have an orchestration workflow, and the entity tracks the execution of business flow by invoking the configured tasks.</p>
<p>At the same time, the tasks are decoupled from the orchestration workflow because the invocations are by events (e.g., using Kafka), and these are Request Events.</p>
<p>Microservice workflow conforms to a design pattern called ""outbound context"", a workflow design pattern that consists of the workflow not having any business data in the execution pipeline.</p>
<p>Microservices attend to the activity by subscribing to the events, resolve the activity and give back control to the workflow by creating a ""response"" event.</p>
<p>The workflow's responsibility only consists of triggering tasks and tracking completion. When initiating activities, the payload only consists of a signalling event received by the microservice, and the payload only consists of the workflow instance id.</p>
<p>The workflow does not receive data from the microservices after completing the activities because the response events do not contain business data.</p>
<p>This way, the microservices workflows do not contribute to the functional coupling among microservices. When a workflow triggers an ""activity-microservice"",</p>
<p>The microservices need data to complete the task and collect the data required to resolve the activity by themselves, which can be solved in different ways.</p>
<p>One of the most elegant and efficient ways of solving the retrieval of data (from the outbound context) is by using the features of Kafka stream joins. It creates a new event given the existence of other events as a SQL-like join condition; for example, when two different events happen, and it is possible to correlate them, the stream-join creates a new event with the payload of the two original ones. So the microservices will be consuming the stream join event created from the workflow signalling event and another event containing the required payload data.</p>
"
NestJS,Adopt,languages-and-frameworks,FALSE,"<p>NestJS</p>
<p>Writing a review after using NestJS for the past two years provides us with the opportunity to emit a complete and thorough opinion. As with any long relationship. We can say that some parts of NestJS amuse and make any developer fall in love. Others can be frustrating, and one could start imagining discovering the &ldquo;new thing&rdquo; after NestJS.</p>
<p>Let&rsquo;s focus on why we love NestJS; it is a framework for building serverside NodeJS applications. NestJS is derived from the philosophy of Angular applications. When we say Angular application, we refer to a loosely coupled, highly scalable architecture for applications. NestJS adopted both TypeScript and Angular languages. The best of TypeScript is that it makes it easy to define data types, and the framework already provides basic and advanced data types. If the developers are experienced, they can use advanced features in TypeScript, for example, inheritance, advanced dependency injection, dynamic modules, and service. These inherited features from Angular help to create large enterprise applications with scalability and modularity.</p>
<p>So, NestJS uses TypeScript and implicitly Angular features, but on the server side to build robust business domains and achieve code base high maintainability.</p>
<p>While NestJS can be used with both plain Javascript and Typescript, Typescript is the obvious choice and the favourite language adopted by the most experienced development team.</p>
<p>NestJS is written in Typescript; therefore, almost all the examples on the documentation are also in Typescript. Typescript is a superior language to plain Javascript (in my opinion), so Typescript is the obvious choice for a project that is not throwaway or very small.</p>
<p>Another thing we like is the quick documentation &amp; health checks. It is implemented with the swagger module, the compodoc integration, nest-raven and nest status monitor.</p>
<p>So it is straightforward to get a basic application up and running with advanced features in minimal time, including error logging, status monitoring and api/module documentation.</p>
<p>The modular approach deserves a special mention., NestJS is very modular, which means that when you create a module, let&rsquo;s say, for example, a global exceptions module, it is possible to plug and play it in every other NestJS service being built.</p>
<p>We think it is a good tool that provides a good return on investment; it greatly rewards a company for investing resources in learning and mastering NestJS. It is possible to have an extensive library and collection of internal tools in a lapse of months, if not weeks. After a while, developing in NestJS feels like putting together lego bricks to construct new buildings.</p>"
NgRx,Assess,languages-and-frameworks,FALSE,"<p>NgRx</p>
<p>When developing complex single-page applications, we frequently need to manage states. Angular does not provide an acceptable in-built support state management library as part of the framework. <br />So, NgRx is the best library we can recommend for Angular state management. NgRx Store provides reactive state management for Angular apps inspired by the Redux pattern. About Redux: ""Redux is a pattern for managing and updating application state, using events called ""actions"". It serves as a centralized store for a state that needs to be used across your entire application, with rules ensuring that the state can only be updated in a predictable fashion"". <br />So we can say that NgRx is the Redux adaptation for Angular.<br />The NgRx concept is straightforward; it has a single universal store that serves as a single source of truth in the application. It provides loose coupling as the modification logic has been taken care of in NgRx Reducers and Effects. <br />Reducers are functions responsible for handling transitions from one state to the next state in an application. Effects are used to perform side effects like fetching data using HTTP, WebSockets, writing to browser storage, and more. In a large application, Effects become more critical because they are used to multiple abstract sources of data from components. Effects handle external data and interactions on behalf of components.<br />If the reader is familiar with micro frontends and a component-based architecture in Angular, then having good state management is critical. <br />The problem arises when there is a need for state modification across the same level and or higher-level components. <br />One could think of solutions without NgRx Redux, for example, using shared services. However, we found this is ineffective, as it drives the solution to have components tightly coupled. In addition, it has a technical problem: data will not change in components if the data reference changes in services. <br />As with any other framework, NgRx has a learning curve for new angular developers and comes with added complexity. So we can recommend NgRx only for large or complex applications.</p>"
Storybook,Adopt,languages-and-frameworks,FALSE,"<p>Storybook</p>
<p>If you have ever worked in a large development team, you have probably noticed that bigger teams are not very efficient. The number of miscommunications grows as the number of team members increases. <br />For the UX team, these reflect the number of reworks; the existing UI patterns go undocumented or are lost altogether. That means developers usually reinvent the wheel once and again instead of focusing on building new business features. Moreover, over time, projects are littered with one-off components that are lost in projects' repositories.<br />So, the thing with Storybook is that it helps us to have some order of all components across projects and repositories. <br />The Storybook Design System codifies existing UI components and manages them in a centralised, well-maintained repository. So, the UI components can be reused across projects, and developers can avoid copying and pasting the same components into multiple projects' code bases. <br />If adequately managed, Storybook can also be a centralised repository of UX design patterns where we can build and test UI components in isolation before placing them in other apps. <br />Projects can reuse design system components and further customise if needed in their local codebase.<br />If the reader is familiar with micro frontends and component-based architecture, then Storybook can be where all the components are sourced and maintained. It can serve as a display catalogue for anyone to browse and reuse components.</p>"
Telepresence microservices debugging,Adopt,Tools,TRUE,"<p> Telepresence microservices debugging</p>
<p>The microservices architecture is conformed by one or more systems which is a distributed system. A set of microservices are used to build a business use case or a business flow; one of the challenges with this type of architecture is debugging one or more of these subsystems. <br />In a traditional single-deployment architecture, it is possible to set up the necessary components of a monolithic system in the developer's local computer and debug the code. However, in a large microservices landscape, setting up several of these systems for debugging on a local desktop is not cost-effective because they require a large amount of memory and computational power, which sometimes is impossible to provide to developers. <br />When using Kubernetes, then there is a solution to this problem. It is proposed by Telepresence, which allows redirecting the traffic from selected microservices from the development environment to the local machine by using a proxy set-up. The proxy is a mediator and is set up as a sidecar. So the execution happens in the server, but only selected components are running locally, and the traffic is redirected accordingly. <br />Telepresence has been built using Ambassador's existing strong networking foundation, which is a network traffic controller for Kubernetes. <br />Now development teams can have an alternative for setting up development environments and local desktops for debugging large distributed interconnected networks of microservices. <br />Our team is trialling this, and one of the challenges was that the Telepresence free licence version only allows one single developer per proxy at a time. If more than one developer needs to work on the same microservice simultaneously, then upgrading to the licenced version would be required. It supports multiple concurrent developers from a single Telepresence proxy set-up by adding a unique id per developer to the traffic header and distributing the traffic accordingly. <br />Otherwise, a workaround can also make this possible, which consists in setting up one Telepresence proxy per namespace. However, each developer must be assigned a different microservices instance with its namespace.</p>"
Vuetify,Adopt,languages-and-frameworks,FALSE,"<p>Vuetify</p>
<p>Developing a great front can be a time-consuming and tedious process of moving things around, resizing, changing color, and so on, and on, and on. It also takes time to build because you have to add multiple dependencies and build from scratch. While creating a custom component in some cases you have to add multiple dependencies which make your component bulky and over-coded.</p>
<p>For the above solution, Vuetify is defining a decent role. We all know that time is money, so Vuetify components are the fastest way to create a front. We as developer-only have to hook functionalities to these components and there is no need to worry about looks or common behavior&hellip; just apply the app&rsquo;s logic and voila! All the styling work that many of us as developers don&rsquo;t like&mdash;especially when we need to support Internet Explorer&mdash; is taken care of. Vuetify provides immense support. You never know what can cause problems and when you&rsquo;ll need support, which is why it&rsquo;s nice to use the most popular or commercial frameworks like Vuetify.&nbsp;</p>
<p>It has a very active development cycle and is patched weekly, responding to community issues and reports at breakneck speed, allowing you to get your hands on bug fixes and enhancements more often.</p>"